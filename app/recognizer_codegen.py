from __future__ import annotations

import hashlib
import re
from collections import OrderedDict
from dataclasses import dataclass
from pathlib import Path
from typing import Iterable


@dataclass(frozen=True)
class RecognizerCodegenSpec:
    name: str
    class_name: str
    entity_type: str
    language: str | None
    description: str | None
    base_score: float | None
    patterns: list[dict]
    context: list[str]
    allow_list: list[str] | None
    deny_list: list[str] | None


def _sanitize_identifier(value: str) -> str:
    cleaned = []
    for ch in value:
        if ch.isalnum() or ch == "_":
            cleaned.append(ch)
        else:
            cleaned.append("_")
    result = "".join(cleaned).strip("_")
    return result or "custom"


def derive_class_name(name: str, entity_type: str) -> str:
    del entity_type
    tokens = re.split(r"[^A-Za-z0-9]+", name)
    pascal = "".join(token[:1].upper() + token[1:] for token in tokens if token)
    if not pascal:
        pascal = "CustomRecognizer"
    if not pascal.endswith("Recognizer"):
        pascal += "Recognizer"
    return _sanitize_identifier(pascal)


def derive_module_filename(class_name: str) -> str:
    normalized = _sanitize_identifier(class_name)
    snake = re.sub(r"(?<!^)(?=[A-Z])", "_", normalized).lower()
    return f"{snake}.py"


def _format_pattern_call(name: str, regex: str, score: str) -> list[str]:
    return [
        "        Pattern(",
        f'            "{name}",',
        f'            r"{regex}",',
        f"            {score},",
        "        ),",
    ]


def generate_recognizer_module(spec: RecognizerCodegenSpec) -> str:
    description = spec.description or ""
    supported_language = (
        f'"{spec.language}"' if spec.language else "None"
    )

    pattern_lines: list[str] = ["    PATTERNS = ["]
    for pattern in spec.patterns:
        name = pattern.get("name") or "Pattern"
        regex = pattern.get("regex") or ""
        score = pattern.get("score", spec.base_score)
        score_value = "None" if score is None else str(score)
        pattern_lines.extend(_format_pattern_call(name, regex, score_value))
    pattern_lines.append("    ]")

    context_lines = ["    CONTEXT = ["]
    for item in spec.context:
        context_lines.append(f'        "{item}",')
    context_lines.append("    ]")
    deny_items = ", ".join(f'"{item}"' for item in (spec.deny_list or []))
    deny_line = (
        f"            deny_list=[{deny_items}],"
        if spec.deny_list
        else None
    )

    doc_body = description.replace('"""', "'''")
    if not doc_body:
        doc_body = "Generated recognizer."

    return "\n".join(
        [
            '"""Custom recognizer generated by Purview Analyser."""',
            "from typing import List, Optional, Tuple",
            "from presidio_analyzer import PatternRecognizer, Pattern",
            "from presidio_analyzer import EntityRecognizer",
            "",
            "",
            f"class {spec.class_name}(PatternRecognizer):",
            '    """',
            f"    {doc_body}",
            '    """',
            "",
            *pattern_lines,
            "",
            *context_lines,
            "",
            "    def __init__(",
            "        self,",
            "        patterns: Optional[List[Pattern]] = None,",
            "        context: Optional[List[str]] = None,",
            f"        supported_language: str = {supported_language},",
            f'        supported_entity: str = "{spec.entity_type}",',
            "        replacement_pairs: Optional[List[Tuple[str, str]]] = None,",
            "        name: Optional[str] = None,",
            "    ):",
            "        self.replacement_pairs = (",
            '            replacement_pairs if replacement_pairs else [("-", ""), (" ", "")]',
            "        )",
            "        patterns = patterns if patterns else self.PATTERNS",
            "        context = context if context else self.CONTEXT",
            "        super().__init__(",
            "            supported_entity=supported_entity,",
            "            patterns=patterns,",
            "            context=context,",
            "            supported_language=supported_language,",
            *( [deny_line] if deny_line else [] ),
            '            name=name or "' + spec.name + '",',
            "        )",
            "",
            "    def validate_result(self, pattern_text: str) -> bool:",
            '        """Optional post-regex validation hook. Override if checksum is needed."""',
            "        _ = EntityRecognizer.sanitize_value(pattern_text, self.replacement_pairs)",
            "        return True",
            "",
        ]
    )


def hash_code(content: str) -> str:
    return hashlib.sha256(content.encode("utf-8")).hexdigest()


def ensure_parent_dir(path: Path) -> None:
    path.parent.mkdir(parents=True, exist_ok=True)


def write_recognizer_module(path: Path, content: str) -> None:
    ensure_parent_dir(path)
    path.write_text(content, encoding="utf-8")


def normalize_subpath(subpath: str | None) -> str:
    if not subpath:
        return ""
    return subpath.strip().strip("/")


def resolve_storage_path(root: Path, subpath: str | None, filename: str) -> Path:
    root_resolved = root.resolve()
    sub = normalize_subpath(subpath)
    target = (root / sub / filename).resolve()
    try:
        target.relative_to(root_resolved)
    except ValueError as exc:
        raise ValueError("Storage path must stay within the configured root.") from exc
    return target


def list_python_files(root: Path) -> Iterable[Path]:
    if not root.exists():
        return []
    return [
        path
        for path in root.rglob("*.py")
        if path.is_file() and path.name != "__init__.py"
    ]


def ensure_package_path(root: Path, subpath: str) -> Path:
    target_dir = (root / subpath).resolve()
    target_dir.mkdir(parents=True, exist_ok=True)

    root_init = root / "__init__.py"
    if not root_init.exists():
        root_init.write_text('"""Predefined recognizers package."""\n', encoding="utf-8")

    current = root.resolve()
    for part in [p for p in subpath.split("/") if p]:
        current = current / part
        init_path = current / "__init__.py"
        if not init_path.exists():
            init_path.write_text(
                f'"""{part.replace("_", " ").title()} recognizers package."""\n',
                encoding="utf-8",
            )
    return target_dir


def _update_all_block(content: str, class_name: str) -> str:
    pattern = re.compile(r"__all__\s*=\s*\[(.*?)\]", re.S)
    match = pattern.search(content)
    if not match:
        if content and not content.endswith("\n"):
            content += "\n"
        return content + f'\n__all__ = [\n    "{class_name}",\n]\n'

    block = match.group(1)
    items = re.findall(r'"([^"]+)"', block)
    ordered = list(OrderedDict.fromkeys(items))
    if class_name not in ordered:
        ordered.append(class_name)
    new_block = "__all__ = [\n" + "".join(f'    "{item}",\n' for item in ordered) + "]"
    return content[: match.start()] + new_block + content[match.end() :]


def _insert_import_in_sub_init(content: str, import_line: str) -> str:
    lines = content.splitlines()
    if import_line in lines:
        return content

    import_indices = [idx for idx, line in enumerate(lines) if line.startswith("from .")]
    if import_indices:
        imports = [lines[idx] for idx in import_indices]
        imports.append(import_line)
        imports = sorted(list(OrderedDict.fromkeys(imports)))
        for idx, line_idx in enumerate(import_indices):
            lines[line_idx] = imports[idx]
        if len(imports) > len(import_indices):
            lines.insert(import_indices[-1] + 1, imports[-1])
        return "\n".join(lines) + "\n"

    insert_idx = 0
    for idx, line in enumerate(lines):
        if line.startswith('"""'):
            insert_idx = idx + 1
            continue
        if line.strip() == "":
            insert_idx = idx + 1
            continue
        break
    lines.insert(insert_idx, import_line)
    return "\n".join(lines) + "\n"


def _insert_import_in_root_init(content: str, import_line: str, dotted_subpath: str) -> str:
    lines = content.splitlines()
    if import_line in lines:
        return content

    section_prefix = f"from .{dotted_subpath}."
    matching = [idx for idx, line in enumerate(lines) if line.startswith(section_prefix)]
    if matching:
        section_lines = [lines[idx] for idx in matching]
        section_lines.append(import_line)
        section_lines = sorted(list(OrderedDict.fromkeys(section_lines)))
        for idx, line_idx in enumerate(matching):
            lines[line_idx] = section_lines[idx]
        if len(section_lines) > len(matching):
            lines.insert(matching[-1] + 1, section_lines[-1])
        return "\n".join(lines) + "\n"

    section_name = dotted_subpath.split(".")[-1].replace("_", " ").title()
    section_comment = f"# {section_name} recognizers"
    section_start = next(
        (idx for idx, line in enumerate(lines) if line.strip() == section_comment),
        None,
    )
    if section_start is not None:
        insert_idx = section_start + 1
        while insert_idx < len(lines) and (
            lines[insert_idx].startswith("from ")
            or lines[insert_idx].startswith("import ")
            or lines[insert_idx].strip() == ""
            or lines[insert_idx].startswith("(")
            or lines[insert_idx].startswith(")")
        ):
            insert_idx += 1
        lines.insert(insert_idx, import_line)
        return "\n".join(lines) + "\n"

    insert_idx = next(
        (idx for idx, line in enumerate(lines) if line.startswith("PREDEFINED_RECOGNIZERS")),
        len(lines),
    )
    lines.insert(insert_idx, import_line)
    return "\n".join(lines) + "\n"


def update_init_exports(
    root: Path, subpath: str, module_filename: str, class_name: str
) -> None:
    ensure_package_path(root, subpath)
    module_name = Path(module_filename).stem

    sub_init = root / subpath / "__init__.py"
    sub_content = sub_init.read_text(encoding="utf-8") if sub_init.exists() else ""
    sub_content = _insert_import_in_sub_init(
        sub_content, f"from .{module_name} import {class_name}"
    )
    sub_content = _update_all_block(sub_content, class_name)
    sub_init.write_text(sub_content, encoding="utf-8")

    dotted = subpath.replace("/", ".")
    root_import = f"from .{dotted}.{module_name} import {class_name}"
    root_init = root / "__init__.py"
    root_content = root_init.read_text(encoding="utf-8") if root_init.exists() else ""
    root_content = _insert_import_in_root_init(root_content, root_import, dotted)
    root_content = _update_all_block(root_content, class_name)
    root_init.write_text(root_content, encoding="utf-8")
