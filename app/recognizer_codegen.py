from __future__ import annotations

import hashlib
import re
from dataclasses import dataclass
from pathlib import Path
from typing import Iterable


@dataclass(frozen=True)
class RecognizerCodegenSpec:
    name: str
    class_name: str
    entity_type: str
    language: str | None
    description: str | None
    base_score: float | None
    patterns: list[dict]
    context: list[str]
    allow_list: list[str] | None
    deny_list: list[str] | None


def _sanitize_identifier(value: str) -> str:
    cleaned = []
    for ch in value:
        if ch.isalnum() or ch == "_":
            cleaned.append(ch)
        else:
            cleaned.append("_")
    result = "".join(cleaned).strip("_")
    return result or "custom"


def derive_class_name(name: str, entity_type: str) -> str:
    del entity_type
    tokens = re.split(r"[^A-Za-z0-9]+", name)
    pascal = "".join(token[:1].upper() + token[1:] for token in tokens if token)
    if not pascal:
        pascal = "CustomRecognizer"
    if not pascal.endswith("Recognizer"):
        pascal += "Recognizer"
    return _sanitize_identifier(pascal)


def derive_module_filename(class_name: str) -> str:
    normalized = _sanitize_identifier(class_name)
    snake = re.sub(r"(?<!^)(?=[A-Z])", "_", normalized).lower()
    return f"{snake}.py"


def _format_pattern_call(name: str, regex: str, score: str) -> list[str]:
    return [
        "        Pattern(",
        f'            "{name}",',
        f'            r"{regex}",',
        f"            {score},",
        "        ),",
    ]


def generate_recognizer_module(spec: RecognizerCodegenSpec) -> str:
    description = spec.description or ""
    supported_language = (
        f'"{spec.language}"' if spec.language else "None"
    )

    pattern_lines: list[str] = ["    PATTERNS = ["]
    for pattern in spec.patterns:
        name = pattern.get("name") or "Pattern"
        regex = pattern.get("regex") or ""
        score = pattern.get("score", spec.base_score)
        score_value = "None" if score is None else str(score)
        pattern_lines.extend(_format_pattern_call(name, regex, score_value))
    pattern_lines.append("    ]")

    context_lines = ["    CONTEXT = ["]
    for item in spec.context:
        context_lines.append(f'        "{item}",')
    context_lines.append("    ]")
    deny_items = ", ".join(f'"{item}"' for item in (spec.deny_list or []))
    deny_line = (
        f"            deny_list=[{deny_items}],"
        if spec.deny_list
        else None
    )

    doc_body = description.replace('"""', "'''")
    if not doc_body:
        doc_body = "Generated recognizer."

    return "\n".join(
        [
            '"""Custom recognizer generated by Purview Analyser."""',
            "from typing import List, Optional, Tuple",
            "from presidio_analyzer import PatternRecognizer, Pattern",
            "from presidio_analyzer import EntityRecognizer",
            "",
            "",
            f"class {spec.class_name}(PatternRecognizer):",
            '    """',
            f"    {doc_body}",
            '    """',
            "",
            *pattern_lines,
            "",
            *context_lines,
            "",
            "    def __init__(",
            "        self,",
            "        patterns: Optional[List[Pattern]] = None,",
            "        context: Optional[List[str]] = None,",
            f"        supported_language: str = {supported_language},",
            f'        supported_entity: str = "{spec.entity_type}",',
            "        replacement_pairs: Optional[List[Tuple[str, str]]] = None,",
            "        name: Optional[str] = None,",
            "    ):",
            "        self.replacement_pairs = (",
            '            replacement_pairs if replacement_pairs else [("-", ""), (" ", "")]',
            "        )",
            "        patterns = patterns if patterns else self.PATTERNS",
            "        context = context if context else self.CONTEXT",
            "        super().__init__(",
            "            supported_entity=supported_entity,",
            "            patterns=patterns,",
            "            context=context,",
            "            supported_language=supported_language,",
            *( [deny_line] if deny_line else [] ),
            '            name=name or "' + spec.name + '",',
            "        )",
            "",
            "    def validate_result(self, pattern_text: str) -> bool:",
            '        """Optional post-regex validation hook. Override if checksum is needed."""',
            "        _ = EntityRecognizer.sanitize_value(pattern_text, self.replacement_pairs)",
            "        return True",
            "",
        ]
    )


def hash_code(content: str) -> str:
    return hashlib.sha256(content.encode("utf-8")).hexdigest()


def ensure_parent_dir(path: Path) -> None:
    path.parent.mkdir(parents=True, exist_ok=True)


def write_recognizer_module(path: Path, content: str) -> None:
    ensure_parent_dir(path)
    path.write_text(content, encoding="utf-8")


def normalize_subpath(subpath: str | None) -> str:
    if not subpath:
        return ""
    return subpath.strip().strip("/")


def resolve_storage_path(root: Path, subpath: str | None, filename: str) -> Path:
    root_resolved = root.resolve()
    sub = normalize_subpath(subpath)
    target = (root / sub / filename).resolve()
    try:
        target.relative_to(root_resolved)
    except ValueError as exc:
        raise ValueError("Storage path must stay within the configured root.") from exc
    return target


def list_python_files(root: Path) -> Iterable[Path]:
    if not root.exists():
        return []
    return [
        path
        for path in root.rglob("*.py")
        if path.is_file() and path.name != "__init__.py"
    ]


def ensure_package_path(root: Path, subpath: str) -> Path:
    target_dir = (root / subpath).resolve()
    target_dir.mkdir(parents=True, exist_ok=True)

    root_init = root / "__init__.py"
    if not root_init.exists():
        root_init.write_text('"""Predefined recognizers package."""\n', encoding="utf-8")

    current = root.resolve()
    for part in [p for p in subpath.split("/") if p]:
        current = current / part
        init_path = current / "__init__.py"
        if not init_path.exists():
            init_path.write_text(
                f'"""{part.replace("_", " ").title()} recognizers package."""\n',
                encoding="utf-8",
            )
    return target_dir


def _append_import_and_all(init_path: Path, import_line: str, class_name: str) -> None:
    content = init_path.read_text(encoding="utf-8") if init_path.exists() else ""
    if import_line not in content:
        if content and not content.endswith("\n"):
            content += "\n"
        content += f"\n{import_line}\n"

    if "__all__" not in content:
        content += f'\n__all__ = [\n    "{class_name}",\n]\n'
    elif f'"{class_name}"' not in content:
        marker = "]"
        idx = content.rfind(marker)
        if idx != -1 and "__all__" in content[:idx]:
            content = content[:idx] + f'    "{class_name}",\n' + content[idx:]
        else:
            content += f'\n__all__ = [\n    "{class_name}",\n]\n'

    init_path.write_text(content, encoding="utf-8")


def update_init_exports(
    root: Path, subpath: str, module_filename: str, class_name: str
) -> None:
    ensure_package_path(root, subpath)
    module_name = Path(module_filename).stem

    sub_init = root / subpath / "__init__.py"
    _append_import_and_all(sub_init, f"from .{module_name} import {class_name}", class_name)

    dotted = subpath.replace("/", ".")
    root_import = f"from .{dotted}.{module_name} import {class_name}"
    root_init = root / "__init__.py"
    _append_import_and_all(root_init, root_import, class_name)
